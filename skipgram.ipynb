{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Implement Skip-Gram Model**"
      ],
      "metadata": {
        "id": "qPzTU2MimSr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Import the Libraries**"
      ],
      "metadata": {
        "id": "7THUAoT-mk1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_kosLEbdE9u"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.core import Reshape"
      ],
      "metadata": {
        "id": "9lX2iu4wdFkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFj8RNgHdHL3",
        "outputId": "004f8f8a-a33f-4d97-bd0d-4b0413772872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n"
      ],
      "metadata": {
        "id": "J_XdIuw4dJQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Build the Model**"
      ],
      "metadata": {
        "id": "f9gdnRX1my5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_document(doc):\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "  tokens = wpt.tokenize(doc)\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "  return doc\n"
      ],
      "metadata": {
        "id": "SJv09xj_dKq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "bible = gutenberg.sents('bible-kjv.txt')\n",
        "remove_terms = punctuation + '0123456789'\n",
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n"
      ],
      "metadata": {
        "id": "Ko9lWXmndNjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "vocab_size = len(word2id) + 1\n",
        "embed_size = 100"
      ],
      "metadata": {
        "id": "oNHGFwHvdQCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "print('Vocabulary Size:', vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_P9B6ldR56",
        "outputId": "e2acabfc-7dc5-4f5e-c523-eb7b4a1526be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 12425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "  context_length = window_size*2\n",
        "  for words in corpus:\n",
        "    sentence_length = len(words)\n",
        "    for index, word in enumerate(words):\n",
        "      context_words = []\n",
        "      label_word = []\n",
        "      start = index - window_size\n",
        "      end = index + window_size + 1\n",
        "      context_words.append([words[i]\n",
        "      for i in range(start, end)\n",
        "      if 0 <= i < sentence_length\n",
        "      and i != index])\n",
        "      label_word.append(word)\n",
        "      x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
        "      y = np_utils.to_categorical(label_word, vocab_size)\n",
        "      yield (x, y)\n",
        "      \n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in\n",
        "wids]\n"
      ],
      "metadata": {
        "id": "mpfjvGZhdTQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Compiling and checking the model accuracy**"
      ],
      "metadata": {
        "id": "DWjob48jm4u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,embeddings_initializer=\"glorot_uniform\",input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,embeddings_initializer=\"glorot_uniform\",input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n"
      ],
      "metadata": {
        "id": "xzVBL2FUdfQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Concatenate([word_model, context_model]))\n",
        "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "for epoch in range(1, 6):\n",
        "  loss = 0\n",
        "for i, elem in enumerate(skip_grams):\n",
        "  pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "  pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "  labels = np.array(elem[1], dtype='int32')\n",
        "  X = [pair_first_elem, pair_second_elem]\n",
        "  Y = labels\n",
        "  if i % 10000 == 0:\n",
        "    print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "  \n",
        "print('Epoch:', epoch, 'Loss:', loss)\n"
      ],
      "metadata": {
        "id": "miQi_bsqduSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}